{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZmXwrJ/ZFlcYrx42fmDh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivayogi-A/Pyspark_programming/blob/master/Pyspark_Joins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnJV5Ce-sv6M",
        "outputId": "b5ba411c-a3c7-46aa-954d-a53bf23eefb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [907 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,553 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,221 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,787 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,396 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,131 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,118 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,421 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,884 kB]\n",
            "Fetched 24.8 MB in 5s (5,336 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# create a spark session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "Spark = SparkSession.builder\\\n",
        "        .appName(\"Studentfilter\")\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark SQL Joins comes with more optimizations by default (thanks to DataFrames), but there are still some performance issues to consider while using it. Understanding how to effectively utilize PySpark joins is essential for conducting comprehensive data analysis, building data pipelines, and deriving valuable insights from large-scale datasets.\n",
        "\n",
        "In this PySpark SQL Join, you will learn different Join syntaxes and use different Join types on two or more DataFrames and Datasets using examples."
      ],
      "metadata": {
        "id": "dvjws5QBtiMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark Join Syntax**\n",
        "\n",
        "* PySpark SQL join has a below syntax and it can be accessed directly from DataFrame."
      ],
      "metadata": {
        "id": "sGaWTHPAtq18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_DF.join(second_DF,first_DF.column_name ==  second_DF.column_name,\"JoinType\")"
      ],
      "metadata": {
        "id": "ctlUvYgRtlxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark Join Types**\n",
        "\n",
        "\n",
        "Below are the different Join Types PySpark supports.\n",
        "\n",
        "**Join String** - - - - - - - - - - - - -  - - - - - - - - - - -**Equivalent SQL Join**\\\n",
        "inner\t - - - - - - - - - - - - - - - - - - - - - - - - -- - - - -INNER JOIN\\\n",
        "outer, full, fullouter, full_outer - - - - - - - - - \tFULL OUTER JOIN\\\n",
        "left, leftouter, left_outer - - - - - - - - - - - - - - \tLEFT JOIN\\\n",
        "right, rightouter, right_outer - - - - - - - - - - - RIGHT JOIN\\\n",
        "cross\\\n",
        "anti, leftanti, left_anti\\\n",
        "semi, leftsemi, left_semi\n",
        "\n",
        "\n",
        "\n",
        "Before diving into PySpark SQL Join illustrations, let’s initiate “emp” and “dept” DataFrames.The emp DataFrame contains the “emp_id” column with unique values, while the dept DataFrame contains the “dept_id” column with unique values. Additionally, the “emp_dept_id” from “emp” refers to the “dept_id” in the “dept” dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "dvOh8eeFu1bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000),\n",
        "       (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000),\n",
        "       (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000),\n",
        "       (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000),\n",
        "       (5,\"Brown\",2,\"2010\",\"40\",\"\",-1),\n",
        "       (6,\"Brown\",2,\"2010\",\"50\",\"\",-1)\n",
        "       ]\n",
        "\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\",\"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "empDF = Spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show()\n",
        "\n",
        "dept = [(\"Finance\",10),\n",
        "        (\"Marketing\",20),\n",
        "        (\"Sales\",30), \\\n",
        "        (\"IT\",40) \\\n",
        "        ]\n",
        "\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "\n",
        "deptDF = Spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZXWzT_BxTJ-",
        "outputId": "8314b9a4-67eb-4912-b185-6254b61ba41b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- superior_emp_id: long (nullable = true)\n",
            " |-- year_joined: string (nullable = true)\n",
            " |-- emp_dept_id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|     1|   Smith|             -1|       2018|         10|     M|  3000|\n",
            "|     2|    Rose|              1|       2010|         20|     M|  4000|\n",
            "|     3|Williams|              1|       2010|         10|     M|  1000|\n",
            "|     4|   Jones|              2|       2005|         10|     F|  2000|\n",
            "|     5|   Brown|              2|       2010|         40|      |    -1|\n",
            "|     6|   Brown|              2|       2010|         50|      |    -1|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|  Finance|     10|\n",
            "|Marketing|     20|\n",
            "|    Sales|     30|\n",
            "|       IT|     40|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How Join works?**\n",
        "\n",
        "PySpark’s join operation combines data from two or more Datasets based on a common column or key. It is a fundamental operation in PySpark and is similar to SQL joins.\n",
        "\n",
        "**Common Key**: In order to join two or more datasets we need a common key or a column on which you want to join. This key is used to join the matching rows from the datasets.\n",
        "\n",
        "**Partitioning**: PySpark Datasets are distributed and partitioned across multiple nodes in a cluster. Ideally, data with the same join key should be located in the same partition. If the Datasets are not already partitioned on the join key, PySpark may perform a shuffle operation to redistribute the data, ensuring that rows with the same join key are on the same node. Shuffling can be an expensive operation, especially for large Datasets.\n",
        "\n",
        "**Join Type Specification**: We can specify the type of join like inner join, full join, left join, etc., by specifying on “how” parameter of the .join() method. This parameter determines which rows should be included or excluded in the resulting Dataset.\n",
        "\n",
        "**Join Execution**: PySpark performs the join by comparing the values in the common key column between the Datasets.\n",
        "\n",
        "**Inner Join**: Returns only the rows with matching keys in both DataFrames.\n",
        "\n",
        "\n",
        "**Left Join**: Returns all rows from the left DataFrame and matching rows from the right DataFrame.\n",
        "\n",
        "\n",
        "**Right Join**: Returns all rows from the right DataFrame and matching rows from the left DataFrame.\n",
        "\n",
        "\n",
        "**Full Outer Join**: Returns all rows from both DataFrames, including matching and non-matching rows.\n",
        "\n",
        "\n",
        "**Left Semi Join**: Returns all rows from the left DataFrame where there is a match in the right DataFrame.\n",
        "\n",
        "\n",
        "**Left Anti Join**: Returns all rows from the left DataFrame where there is no match in the right DataFrame.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l1IouYy41BLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **PySpark Inner Join DataFrame**\n",
        "\n",
        "The default join in PySpark is the inner join, commonly used to retrieve data from two or more DataFrames based on a shared key. An Inner join combines two DataFrames based on the key (common column) provided and results in rows where there is a matching found. Rows from both DataFrames are dropped with a non-matching key."
      ],
      "metadata": {
        "id": "FEeJ8WWO2YfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_df = empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, \"inner\")\n",
        "in_df.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKXuPvxT2qwW",
        "outputId": "2a2cc569-5ebf-4d3b-8325-16195413fc9b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example drops “emp_dept_id” with value 50 from “emp” And “dept_id” with value 30 from “dept” datasets. Following is the result of the above Join statement."
      ],
      "metadata": {
        "id": "oKRTJCE7T365"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **PySpark Left Outer Join**\n",
        "\n",
        "Left a.k.a Leftouter join returns all rows from the left dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found."
      ],
      "metadata": {
        "id": "nH8yUwQOUBEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "left_df = empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, \"left\")\n",
        "left_df.show(truncate = False)\n",
        "\n",
        "## another way to write the same expression is\n",
        "\"\"\"empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\")\n",
        "    .show(truncate=False)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "2snOT8BTUGFq",
        "outputId": "954225cf-1082-4bb8-ceac-a8539c07629a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\")\\n    .show(truncate=False)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our dataset, the record with “emp_dept_id” 50 does not have a corresponding entry in the “dept” dataset, resulting in null values in the “dept” columns (dept_name & dept_id). Additionally, the entry with “dept_id” 30 from the “dept” dataset is omitted from the results."
      ],
      "metadata": {
        "id": "-KYJZ7MuWAes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Right Outer Join**\n",
        "\n",
        "Right a.k.a Rightouter join is opposite of left join, here it returns all rows from the right dataset regardless of math found on the left dataset, when join expression doesn’t match, it assigns null for that record and drops records from left where match not found."
      ],
      "metadata": {
        "id": "6jlYByXzWMcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "right_df = empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"right\")\n",
        "right_df.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S-KUBpdWS7u",
        "outputId": "d22a28ce-719d-4410-bf3c-f8dbfbbf2401"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our example, the dataset on the right, containing “dept_id” with a value of 30, does not have a corresponding record in the left dataset “emp”. Consequently, this record contains null values for the columns from “emp”. Additionally, the record with “emp_dept_id” value 50 is dropped as no match was found in the left dataset. Below is the result of the aforementioned join expression."
      ],
      "metadata": {
        "id": "wdSa2auyXPiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full Outer Join**\n",
        "\n",
        "Outer a.k.a full, fullouter join in PySpark combines the results of both left and right outer joins, ensuring that all records from both DataFrames are included in the resulting DataFrame. It includes all rows from both DataFrames and fills in missing values with nulls where there is no match. In other words, it merges the DataFrames based on a common key, but retains all rows from both DataFrames, even if there’s no match. This join type is useful when you want to preserve all the information from both datasets, regardless of whether there’s a match on the key or not."
      ],
      "metadata": {
        "id": "j49dDtS_XRHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_outer_df = empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"outer\")\n",
        "full_outer_df.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B33Sah7BXO75",
        "outputId": "14422ce6-6868-4753-8a58-d930ab8dfc70"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet performs a full outer join between two PySpark DataFrames, empDF and deptDF, based on the condition that emp_dept_id from empDF is equal to dept_id from deptDF.\n",
        "\n",
        "In our “emp” dataset, the “emp_dept_id” with a value of 50 does not have a corresponding record in the “dept” dataset, resulting in null values in the “dept” columns. Similarly, the “dept_id” 30 does not have a record in the “emp” dataset, hence you observe null values in the “emp” columns."
      ],
      "metadata": {
        "id": "VlbIIAJkYaAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left Semi Join**\\\n",
        "A Left Semi Join in PySpark returns only the rows from the left DataFrame (the first DataFrame mentioned in the join operation) where there is a match with the right DataFrame (the second DataFrame). It does not include any columns from the right DataFrame in the resulting DataFrame. This join type is useful when you only want to filter rows from the left DataFrame based on whether they have a matching key in the right DataFrame.\n",
        "\n",
        "Left Semi Join can also be achieved by selecting only the columns from the left dataset from the result of the inner join"
      ],
      "metadata": {
        "id": "9aFa_2gvYiCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "left_semi_df = empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, \"left_semi\")\n",
        "left_semi_df.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUIvPSh_YOv6",
        "outputId": "b8edc45e-78d7-4478-e4ca-90c5466ec5c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left Anti Join**\\\n",
        "A Left Anti Join in PySpark returns only the rows from the left DataFrame (the first DataFrame mentioned in the join operation) where there is no match with the right DataFrame (the second DataFrame). It excludes any rows from the left DataFrame that have a corresponding key in the right DataFrame. This join type is useful when you want to filter out rows from the left DataFrame that have matching keys in the right DataFrame."
      ],
      "metadata": {
        "id": "J6-i-kyvZa3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "left_anti_df = empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"left_anti\")\n",
        "left_anti_df.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BY0ke9aZdNO",
        "outputId": "1058dd99-a6d3-47d1-c99a-166c4f627634"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|6     |Brown|2              |2010       |50         |      |-1    |\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using SQL Expression**\\\n",
        "Alternatively, you can also use SQL query to join DataFrames/tables in PySpark. To do so, first, create a temporary view using createOrReplaceTempView(), then use the spark.sql() to execute the join query."
      ],
      "metadata": {
        "id": "eX54gffXar0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empDF.createOrReplaceTempView(\"EMP\")\n",
        "deptDF.createOrReplaceTempView(\"DEPT\")\n",
        "\n",
        "joinDF = Spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "joinDF2 = Spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FjHcTGR4aufC",
        "outputId": "87ae4bf8-07a1-41eb-c133-5bbcd8228fd2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark SQL Join on multiple DataFrames**\\\n",
        "When you need to join more than two tables, you either use SQL expression after creating a temporary view on the DataFrame or use the result of join operation to join with another DataFrame like chaining them. for example"
      ],
      "metadata": {
        "id": "XCnIA1-Yk-0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# syntax for joining multiple joins\n",
        "df1.join(df2,df1.id1 == df2.id2,\"inner\") \\\n",
        "   .join(df3,df1.id1 == df3.id3,\"inner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMoQHSyPbDsU",
        "outputId": "154db36f-0ef2-427c-e88a-fb962ac654a6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frequently asked questions on PySpark Joins**\\\n",
        "**What is the default join in PySpark?**\\\n",
        "In PySpark the default join type is “inner” join when using with .join() method. If you don’t explicitly specify the join type using the “how” parameter, it will perform the inner join. One can change the join type using the how parameter of .join().\n",
        "\n",
        "**Is join expensive in PySpark?**\\\n",
        "Yes, Join in PySpark is expensive because of the data shuffling (wider transformation) that happens between the partitioned data in a cluster. It basically depends on the data size, data skew, cluster configuration, join type being performed, partitioning, and broadcast joins.\n",
        "\n",
        "**Can we join on multiple columns in PySpark?**\\\n",
        "Yes, we can join on multiple columns. Joining on multiple columns involves more join conditions with multiple keys for matching the rows between the datasets.It can be achieved by passing a list of column names as the join condition when using the .join() method.\n",
        "\n",
        "**How do I drop duplicate columns after joining PySpark?**\\\n",
        "PySpark distinct() function is used to drop/remove the duplicate rows (all columns) from Dataset and dropDuplicates() is used to drop rows based on selected (one or multiple) columns\n",
        "\n",
        "**What is the difference between the inner join and the left join?**\\\n",
        "The key difference is that an inner join includes only the rows with matching keys in both Datasets, while a left join includes all the rows from the left Dataset and matches them with rows from the right Dataset where there’s a match. Non-matching rows in the left Dataset in a left join are included with null values in the columns from the right Dataset.\n",
        "\n",
        "**What is the difference between left join and left outer join?**\\\n",
        "Both terms refer to the same type of join operation, and they can be used interchangeably. The “OUTER” keyword is optional when specifying a “LEFT JOIN.”"
      ],
      "metadata": {
        "id": "eAGJEr9zma3g"
      }
    }
  ]
}