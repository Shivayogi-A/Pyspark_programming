{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPgvf5Rj0v6GUz5H/4YS5M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivayogi-A/Pyspark_programming/blob/master/partitionBy()_and_BucketBy().ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark PartitionBy()\n",
        "\n",
        "PySpark partitionBy() is a function which is used to partition the DataFrame into smaller files based on one or multiple key columns while writing to disk.\n",
        "\n",
        "PySpark supports partition in two ways; **partition in memory** (DataFrame) and **partition on the disk** (File system).\n",
        "\n",
        "\n",
        "**Partition in memory:** You can partition or repartition the DataFrame by calling repartition() or coalesce() transformations.\n",
        "\n",
        "**Partition on disk:** While writing the PySpark DataFrame back to disk, you can choose how to partition the data based on columns using partitionBy().\n",
        "\n",
        "**Note:** The partitionBy() is available in **DataFrameWriter** class hence, it is used to write the partition data to the disk."
      ],
      "metadata": {
        "id": "TG3byAf20fgz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU5dXAu20eLC"
      },
      "outputs": [],
      "source": [
        "#Syntax of partitionBy()\n",
        "--partitionBy(\"key column name\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example usage of partitionBy()\n",
        "\n",
        "df.write.option(\"header\",True) \\\n",
        "        .partitionBy(\"state\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .csv(\"/tmp/zipcodes-state\")"
      ],
      "metadata": {
        "id": "pg8GVSu_3OqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code will write the data partitioned based on key colmun 'state'."
      ],
      "metadata": {
        "id": "lQ1kdunR4IyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#partitionBy() multiple columns\n",
        "df.write.option(\"header\",True) \\\n",
        "        .partitionBy(\"state\",\"city\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .csv(\"/tmp/zipcodes-state\")"
      ],
      "metadata": {
        "id": "e32ANe5s4b4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code will write date partitioned based on key column 'state' and 'city'\n",
        "\n",
        "\n",
        "\n",
        "**Data Skew â€“ Control Number of Records per Partition File**\n",
        "\n",
        "Use option **maxRecordsPerFile** if you want to control the number of records for each partition. This is particularly helpful when your data is skewed (Having some partitions with very low records and other partitions with a high number of records)."
      ],
      "metadata": {
        "id": "QyYPuSLM4cy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#partitionBy() control number of partitions\n",
        "df.write.option(\"header\",True) \\\n",
        "        .option(\"maxRecordsPerFile\", 2) \\\n",
        "        .partitionBy(\"state\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .csv(\"/tmp/zipcodes-state\")"
      ],
      "metadata": {
        "id": "zlTvNrbz5s71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does partitioning affect query performance?**\\\n",
        "Partitioning can significantly improve query performance, especially when querying specific subsets of data. It helps skip irrelevant data when reading, reducing the amount of data that needs to be processed.\n",
        "\n",
        "**How is partitionBy() different from groupBy() in PySpark?**\\\n",
        "partitionBy() is used for physically organizing data on disk when writing to a file system, while groupBy() is used for the logical grouping of data within a DataFrame."
      ],
      "metadata": {
        "id": "qFZcvIFv6SDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bucketing in Spark - BucketBy()\n",
        "\n",
        "Bucketing is a way to assign rows of a dataset to specific buckets and collocate them on disk.\\\n",
        "This enables efficient wide transformations in Spark, as the data is already collocated in the executors correctly.\\\n",
        "**Wide transformations** are operations that require shuffling data across partitions, which can be a costly operation.\n",
        "\n",
        "In Spark, bucketing is implemented by the **.bucketBy()** method of the **DataFrameWriter** class. To bucket a dataset, you need to provide the method with the number of buckets you want to create and the column to bucket by.\n",
        "\n",
        "Here is an example of how to bucket a dataset in Spark:"
      ],
      "metadata": {
        "id": "o-mHbcYm6bRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"BucketingExample\").getOrCreate()\n",
        "\n",
        "\n",
        "# Load a dataset\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/dataset\")\n",
        "\n",
        "\n",
        "# Bucket the dataset by the \"id\" column into 10 buckets\n",
        "df.write.bucketBy(10, \"id\").sortBy(\"id\").format(\"parquet\").save(\"path/to/bucketed/dataset\")"
      ],
      "metadata": {
        "id": "WOXau_t_8Xhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, we loaded a dataset and bucketed it by the \"id\" column into 10 buckets using the .bucketBy() method. The resulting bucketed dataset is then sorted by the \"id\" column and saved as a parquet file in the specified directory.\n",
        "\n",
        "**When to use partitioning and bucketing?**\\\n",
        "If you will often perform filtering on a given column and it is of low cardinality, partition on that column. If you will be performing complex operations like joins, groupBys, and windowing and the column is of high cardinality, consider bucketing on that column.\\\n",
        "However, bucketing is complicated and requires careful consideration of nuances and caveats. For example, there are conditions that need to be met between two datasets in order for bucketing to have the desired effect. Additionally, bucketing can only be used when the data is saved as a table, as the metadata of the buckets needs to be saved somewhere, usually in a Hive metadata store.\n",
        "Conclusion"
      ],
      "metadata": {
        "id": "h0j3ronG8mWr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LtU4PWh9f56"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}