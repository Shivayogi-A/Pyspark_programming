{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNztKZQpxY6tl5fQcqZjSxj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivayogi-A/Pyspark_programming/blob/master/Caching_in_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAy-RXr8cdPn",
        "outputId": "92f45864-b618-4a57-8746-7409d49ae8be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,131 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,787 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,118 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [907 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,221 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,553 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,421 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,884 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,396 kB]\n",
            "Fetched 24.8 MB in 7s (3,333 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# create a spark session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "Spark = SparkSession.builder\\\n",
        "        .appName(\"Studentfilter\")\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pyspark cache()**\\\n",
        "Pyspark cache() method is used to cache the intermediate results of the transformation so that other transformation runs on top of cached will perform faster. Caching the result of the transformation is one of the optimization tricks to improve the performance of the long-running PySpark applications/jobs.\n",
        "\n",
        "cache() is a lazy evaluation in PySpark meaning it will not cache the results until you call the action operation. In this article, I will explain what is cache, how it improves performance, and how to cache PySpark DataFrame results with examples.\n",
        "\n",
        "**Benefits of Caching**\\\n",
        "Caching a DataFrame that can be reused for multi-operations will significantly improve any PySpark job. Below are the benefits of cache().\n",
        "\n",
        "**Cost-efficient** – Spark computations are very expensive hence reusing the computations are used to save cost.\\\n",
        "**Time-efficient** – Reusing repeated computations saves lots of time.\\\n",
        "**Execution time** – Saves execution time of the job and we can perform more jobs on the same cluster.\n",
        "\n"
      ],
      "metadata": {
        "id": "sz4DFtTSdH1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do we need Cache in PySpark?**\\\n",
        "First, let’s run some transformations without cache and understand what is the performance issue.\n",
        "\n"
      ],
      "metadata": {
        "id": "3V_np6rueMLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_data = [( 894954, 'Shiva','AIA','Bengaluru'),\n",
        "        ( 894941, 'Vaishnavi','AIA','Bengaluru'),\n",
        "         ( 894950, 'Suraj','AIA','Pune'),\n",
        "        ( 894921, 'Shubham','DEV','Chennai'),\n",
        "        ( 894930, 'Gautam','DEV','Pune'),\n",
        "        (894900, 'Abhijit','AIA','Bengaluru')\n",
        "]\n",
        "\n",
        "schema = [\"emp_id\",\"name\",\"domain\",\"location\"]\n",
        "\n",
        "emp_df = Spark.createDataFrame(emp_data, schema = schema,)\n",
        "emp_df.show(truncate = False)\n",
        "emp_df.printSchema()\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "df2 = emp_df.where(col(\"domain\")=='AIA')\n",
        "df2.show()\n",
        "\n",
        "df3 = df2.where(col(\"location\")=='Bengaluru')\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ1ejyxZeUS_",
        "outputId": "8f1dba7a-a939-4972-c673-bd331e17c207"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+------+---------+\n",
            "|emp_id|name     |domain|location |\n",
            "+------+---------+------+---------+\n",
            "|894954|Shiva    |AIA   |Bengaluru|\n",
            "|894941|Vaishnavi|AIA   |Bengaluru|\n",
            "|894950|Suraj    |AIA   |Pune     |\n",
            "|894921|Shubham  |DEV   |Chennai  |\n",
            "|894930|Gautam   |DEV   |Pune     |\n",
            "|894900|Abhijit  |AIA   |Bengaluru|\n",
            "+------+---------+------+---------+\n",
            "\n",
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- domain: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            "\n",
            "+------+---------+------+---------+\n",
            "|emp_id|     name|domain| location|\n",
            "+------+---------+------+---------+\n",
            "|894954|    Shiva|   AIA|Bengaluru|\n",
            "|894941|Vaishnavi|   AIA|Bengaluru|\n",
            "|894950|    Suraj|   AIA|     Pune|\n",
            "|894900|  Abhijit|   AIA|Bengaluru|\n",
            "+------+---------+------+---------+\n",
            "\n",
            "+------+---------+------+---------+\n",
            "|emp_id|     name|domain| location|\n",
            "+------+---------+------+---------+\n",
            "|894954|    Shiva|   AIA|Bengaluru|\n",
            "|894941|Vaishnavi|   AIA|Bengaluru|\n",
            "|894900|  Abhijit|   AIA|Bengaluru|\n",
            "+------+---------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s assume you have billions of records as in the above example in company db. Since action triggers the transformations, in the above example df2.show() is the first action hence it triggers the execution of reading data from **emp_data** list, and then df.where().\n",
        "\n",
        "We also have another action df3.show(), this again triggers execution of reading data from **emp_data** list, df.where() and df2.where().\n",
        "\n",
        "So in the above example, we are reading the data twice and df.where() twice. when you are detailing large number of records, this will become a performance issue and it can be easily avoided by caching the results of spark.read() and df2.where() as below.\n",
        "\n"
      ],
      "metadata": {
        "id": "MURwn4ufh2J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Syntax of cache()\n",
        "\n",
        "DataFrame.cache()"
      ],
      "metadata": {
        "id": "b1aDAmN4jkiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using PySpark Cache**\\\n",
        "From the above example, let’s add cache() statement to spark.read() and df.where() transformations. When df2.show() executes, this triggers spark.createDataframe(..).cache() which reads the data from **emp_data** and caches the result in memory. and df.where(..).cache() also caches the result in memory.\n",
        "\n",
        "When df3.show() executes, it just performs the df2.where() on top of cache results of df2, without re-executing previous transformations."
      ],
      "metadata": {
        "id": "SgfFBv46jsvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df = Spark.createDataFrame(emp_data, schema = schema,).cache()\n",
        "emp_df.show(truncate = False)\n",
        "\n",
        "df2 = emp_df.where(col(\"domain\")=='AIA').cache()\n",
        "df2.show()\n",
        "\n",
        "df3 = df2.where(col(\"location\")=='Bengaluru')\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsDbeMFegV_8",
        "outputId": "6198208d-576d-4237-a543-2fc18dde2fac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+------+---------+\n",
            "|emp_id|name     |domain|location |\n",
            "+------+---------+------+---------+\n",
            "|894954|Shiva    |AIA   |Bengaluru|\n",
            "|894941|Vaishnavi|AIA   |Bengaluru|\n",
            "|894950|Suraj    |AIA   |Pune     |\n",
            "|894921|Shubham  |DEV   |Chennai  |\n",
            "|894930|Gautam   |DEV   |Pune     |\n",
            "|894900|Abhijit  |AIA   |Bengaluru|\n",
            "+------+---------+------+---------+\n",
            "\n",
            "+------+---------+------+---------+\n",
            "|emp_id|     name|domain| location|\n",
            "+------+---------+------+---------+\n",
            "|894954|    Shiva|   AIA|Bengaluru|\n",
            "|894941|Vaishnavi|   AIA|Bengaluru|\n",
            "|894950|    Suraj|   AIA|     Pune|\n",
            "|894900|  Abhijit|   AIA|Bengaluru|\n",
            "+------+---------+------+---------+\n",
            "\n",
            "+------+---------+------+---------+\n",
            "|emp_id|     name|domain| location|\n",
            "+------+---------+------+---------+\n",
            "|894954|    Shiva|   AIA|Bengaluru|\n",
            "|894941|Vaishnavi|   AIA|Bengaluru|\n",
            "|894900|  Abhijit|   AIA|Bengaluru|\n",
            "+------+---------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we are reading data into DataFrame emp_df. Applying where transformation on emp_df will result in df2 that contains only records where domain=\"AIA\" and caching this DataFrame. As discussed cache() will not perform the transformation as they are lazy in nature. When df2.show() executed then only the code where(col(domain) ==\"AIA\").cache() will be evaluated and caches the result into df2..\n",
        "\n",
        "By applying where transformation on df2 with location=Bengaluru, since the df2 is already cached, the spark will look for the data that is cached and thus uses that DataFrame. Above is the output after performing a transformation on df2 which is read into df3, then applying action show()."
      ],
      "metadata": {
        "id": "WfPKYvUPkZ0t"
      }
    }
  ]
}