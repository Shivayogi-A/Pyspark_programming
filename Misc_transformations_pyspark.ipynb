{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1URCnqylSYKFjQQJiDQSb9shQwiFD2y33",
      "authorship_tag": "ABX9TyOVzpPB3QI4RSyU8g9l93AJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivayogi-A/Pyspark_programming/blob/master/Misc_transformations_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "eCoyUpT3JMeZ",
        "outputId": "4fef9d78-40a6-4bac-c747-6f1bbde6ac28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com] [1 InRelease 11.3 kB/129 kB 9%] [Waiting for headers] [Connect\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 261 kB in 2s (160 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "drive\t     spark-3.1.1-bin-hadoop3.2\t    spark-3.1.1-bin-hadoop3.2.tgz.1\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x791974da02e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ec4d674cf12e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = [('Shiva', '17','11', '97'),\n",
        "             ('Vaishnavi', '16','03','98'),\n",
        "             ('Suraj', '15','06','1998'),\n",
        "             ('Shubham', '19','08', '1998'),\n",
        "             ('Shiva','17','11','1997')]\n",
        "\n",
        "raw_df = spark.createDataFrame(data_list)"
      ],
      "metadata": {
        "id": "5MNzsD_3WA32"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.show()"
      ],
      "metadata": {
        "id": "8S8EMYYKXp8l",
        "outputId": "5d4f76eb-cb50-4119-9927-1a2f1a6071d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+---+----+\n",
            "|       _1| _2| _3|  _4|\n",
            "+---------+---+---+----+\n",
            "|    Shiva| 17| 11|  97|\n",
            "|Vaishnavi| 16| 03|  98|\n",
            "|    Suraj| 15| 06|1998|\n",
            "|  Shubham| 19| 08|1998|\n",
            "|    Shiva| 17| 11|1997|\n",
            "+---------+---+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pyspark.sql.DataFrame.toDF**\n",
        "\n",
        "DataFrame.toDF(*cols: str)\n",
        "It Returns a new DataFrame that with new specified column names"
      ],
      "metadata": {
        "id": "160R3GQ5teZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df =  raw_df.toDF('Name','Date','Month','Year').repartition(3)\n",
        "new_df.printSchema()\n",
        "new_df.show()"
      ],
      "metadata": {
        "id": "UnK-QXRCXtdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7186f71d-5823-40c2-e711-9a693a4c31ed"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            "\n",
            "+---------+----+-----+----+\n",
            "|     Name|Date|Month|Year|\n",
            "+---------+----+-----+----+\n",
            "|    Suraj|  15|   06|1998|\n",
            "|Vaishnavi|  16|   03|  98|\n",
            "|  Shubham|  19|   08|1998|\n",
            "|    Shiva|  17|   11|  97|\n",
            "|    Shiva|  17|   11|1997|\n",
            "+---------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CASE Statement**\n",
        "\n",
        "Case clause uses a rule to return a specific result based on the specified condition, similar to if/else statements in other programming languages"
      ],
      "metadata": {
        "id": "LbT5U36r32T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df1 = new_df.withColumn(\"Year\",expr(\"\"\"\n",
        "              CASE WHEN Year < 21 THEN Year+2000\n",
        "              WHEN Year < 100 THEN Year+1900\n",
        "              ELSE Year\n",
        "              END\"\"\"))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YzudXZFrujt",
        "outputId": "2ad8265c-ed63-4b1b-dcde-b4beb30001fd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+------+\n",
            "|     Name|Date|Month|  Year|\n",
            "+---------+----+-----+------+\n",
            "|    Suraj|  15|   06|  1998|\n",
            "|Vaishnavi|  16|   03|1998.0|\n",
            "|  Shubham|  19|   08|  1998|\n",
            "|    Shiva|  17|   11|1997.0|\n",
            "|    Shiva|  17|   11|  1997|\n",
            "+---------+----+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The year column is string, but we are performing an arithmetic operation over it. Becuase of it spark will automatically promote it to a decimal data type and once the calucation is completed, again changes it to a string value as the column datatype is string. But the decimal points remain.\n",
        "\n",
        "To fix it there are 2 methods,\n",
        "1. **Inline casting**- cast the column to a relevant data type when you are performing an action.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BliLsyjq4_G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df1 = new_df.withColumn(\"Year\",expr(\"\"\"\n",
        "              CASE WHEN Year < 21 THEN cast(Year as int)+2000\n",
        "              WHEN Year < 100 THEN cast(Year as int)+1900\n",
        "              ELSE Year\n",
        "              END\"\"\"))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrDbi5HTryVj",
        "outputId": "23c16340-1ad8-44aa-cb8e-330736b4e8c7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+----+\n",
            "|     Name|Date|Month|Year|\n",
            "+---------+----+-----+----+\n",
            "|    Suraj|  15|   06|1998|\n",
            "|Vaishnavi|  16|   03|1998|\n",
            "|  Shubham|  19|   08|1998|\n",
            "|    Shiva|  17|   11|1997|\n",
            "|    Shiva|  17|   11|1997|\n",
            "+---------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the above method or you can cast the entire expression into integertype as shown below."
      ],
      "metadata": {
        "id": "x6sxbtyC-82Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "df1  = new_df.withColumn(\"Year\",expr(\"\"\"\n",
        "              CASE WHEN Year < 21 THEN Year+2000\n",
        "              WHEN Year < 100 THEN Year+1900\n",
        "              ELSE Year\n",
        "              END\"\"\").cast(IntegerType()))\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCIkbxlm56Y1",
        "outputId": "ce869eba-df26-40f5-9b07-136610ad425b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+----+\n",
            "|     Name|Date|Month|Year|\n",
            "+---------+----+-----+----+\n",
            "|    Suraj|  15|    6|1998|\n",
            "|Vaishnavi|  16|    3|  98|\n",
            "|  Shubham|  19|    8|1998|\n",
            "|    Shiva|  17|   11|  97|\n",
            "|    Shiva|  17|   11|1997|\n",
            "+---------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Change the schema**: We need to change the schema of the columns,or provide appropriate data types during the dataframe creation phase itself.\n",
        "Changing the schema is a choice, we have to see if it is okay to change the schema and will not affect anything.\n",
        "\n",
        "In the given scenario, changing the schema is perfectly fine."
      ],
      "metadata": {
        "id": "jf77ucou7RM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = new_df.withColumn('Date',col('Date').cast(IntegerType()))\\\n",
        "            .withColumn('Month',col('Month').cast(IntegerType()))\\\n",
        "            .withColumn('Year',col('Year').cast(IntegerType()))\n",
        "\n",
        "\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBqgDtEW7QGk",
        "outputId": "50fb7431-8bb5-4d07-c4ac-930c05aad5e8"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+----+\n",
            "|     Name|Date|Month|Year|\n",
            "+---------+----+-----+----+\n",
            "|    Suraj|  15|    6|1998|\n",
            "|Vaishnavi|  16|    3|  98|\n",
            "|  Shubham|  19|    8|1998|\n",
            "|    Shiva|  17|   11|  97|\n",
            "|    Shiva|  17|   11|1997|\n",
            "+---------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2.withColumn(\"Year\",expr(\"\"\"\n",
        "              CASE WHEN Year < 21 THEN Year+2000\n",
        "              WHEN Year < 100 THEN Year+1900\n",
        "              ELSE Year\n",
        "              END\"\"\"))\n",
        "df3.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfYkmb9o6KgJ",
        "outputId": "77ad1a66-b0ba-48cc-85a7-87087ae62c8e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+----+\n",
            "|     Name|Date|Month|Year|\n",
            "+---------+----+-----+----+\n",
            "|    Suraj|  15|    6|1998|\n",
            "|Vaishnavi|  16|    3|1998|\n",
            "|  Shubham|  19|    8|1998|\n",
            "|    Shiva|  17|   11|1997|\n",
            "|    Shiva|  17|   11|1997|\n",
            "+---------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: It is always recommended to cast the datatypes for columns explicitly to avoid abnormal behaviour of code.**"
      ],
      "metadata": {
        "id": "2bDstNg6Oe4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "concatinating 3 cloumns(Date,Month,Year) to a single column to get date of birth.\n",
        "\n",
        "We can use to_date finction to do this task"
      ],
      "metadata": {
        "id": "gnAy7dw7TsG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = df3.withColumn('DOB', expr(\"to_date(concat(Date, '/',Month, '/', Year),'d/M/y')\"))\n",
        "df4.show()"
      ],
      "metadata": {
        "id": "sPvDM_AV-mIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8728f013-9256-43a5-875f-b0de93d821da"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+----+----------+\n",
            "|     Name|Date|Month|Year|       DOB|\n",
            "+---------+----+-----+----+----------+\n",
            "|    Suraj|  15|    6|1998|1998-06-15|\n",
            "|Vaishnavi|  16|    3|1998|1998-03-16|\n",
            "|  Shubham|  19|    8|1998|1998-08-19|\n",
            "|    Shiva|  17|   11|1997|1997-11-17|\n",
            "|    Shiva|  17|   11|1997|1997-11-17|\n",
            "+---------+----+-----+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got the date of birth, now the date, month and year columns become redundant. Hence we can remove them using drop function.\n"
      ],
      "metadata": {
        "id": "PdlcIBiWVvrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = df4.drop('Date','Month','Year')\n",
        "df5.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyPe-7DwVKM7",
        "outputId": "d889f7c1-000d-4a7b-ccc2-9f1c0c316e16"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+\n",
            "|     Name|       DOB|\n",
            "+---------+----------+\n",
            "|    Suraj|1998-06-15|\n",
            "|Vaishnavi|1998-03-16|\n",
            "|  Shubham|1998-08-19|\n",
            "|    Shiva|1997-11-17|\n",
            "|    Shiva|1997-11-17|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you see the above results we have duplicates, we can use **dropDuplicates()** function to do the task.\n",
        "\n",
        "**dropDuplicates()** : Return a new DataFrame with duplicate rows removed,\n",
        "optionally only considering certain columns."
      ],
      "metadata": {
        "id": "XpEHnXZ6XwEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df6 = df5.dropDuplicates()\n",
        "df6.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DzGf0QQWHxq",
        "outputId": "63fb7b33-dc65-4687-d8c1-9b306cb41c7f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+\n",
            "|     Name|       DOB|\n",
            "+---------+----------+\n",
            "|    Suraj|1998-06-15|\n",
            "|    Shiva|1997-11-17|\n",
            "|  Shubham|1998-08-19|\n",
            "|Vaishnavi|1998-03-16|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTIBSk-dYbzb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}