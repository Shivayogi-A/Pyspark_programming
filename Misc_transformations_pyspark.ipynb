{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1URCnqylSYKFjQQJiDQSb9shQwiFD2y33",
      "authorship_tag": "ABX9TyNItrjPfNANSf6f4aOUQJYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivayogi-A/Pyspark_programming/blob/master/Misc_transformations_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "eCoyUpT3JMeZ",
        "outputId": "d10a3eb3-056c-4446-d369-b4d143aa6644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,208 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,549 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,130 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,780 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,112 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,877 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,390 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,421 kB]\n",
            "Fetched 23.9 MB in 8s (3,044 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "drive  sample_data  spark-3.1.1-bin-hadoop3.2  spark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x791974da02e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ec4d674cf12e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = [('Shiva', '17','11', '97'),\n",
        "             ('Vaishnavi', '16','03','98'),\n",
        "             ('Suraj', '15','06','1998'),\n",
        "             ('Shubham', '19','08', '1998')]\n",
        "\n",
        "raw_df = spark.createDataFrame(data_list)"
      ],
      "metadata": {
        "id": "5MNzsD_3WA32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.show()"
      ],
      "metadata": {
        "id": "8S8EMYYKXp8l",
        "outputId": "858fb27f-f457-4955-f0c7-1cda2fe250c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+---+----+\n",
            "|       _1| _2| _3|  _4|\n",
            "+---------+---+---+----+\n",
            "|    Shiva| 17| 11|  97|\n",
            "|Vaishnavi| 16| 03|  98|\n",
            "|    Suraj| 15| 06|1998|\n",
            "|  Shubham| 19| 08|1998|\n",
            "+---------+---+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pyspark.sql.DataFrame.toDF**\n",
        "\n",
        "DataFrame.toDF(*cols: str)\n",
        "It Returns a new DataFrame that with new specified column names"
      ],
      "metadata": {
        "id": "160R3GQ5teZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df =  raw_df.toDF('Name','Date','Month','Year').repartition(3)\n",
        "new_df.printSchema()\n",
        "new_df.show()"
      ],
      "metadata": {
        "id": "UnK-QXRCXtdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e85746-2210-46dc-be93-ed02aac6c483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            "\n",
            "+---------+----+-----+----+\n",
            "|     Name|Date|Month|Year|\n",
            "+---------+----+-----+----+\n",
            "|Vaishnavi|  16|   03|  98|\n",
            "|  Shubham|  19|   08|1998|\n",
            "|    Shiva|  17|   11|  97|\n",
            "|    Suraj|  15|   06|1998|\n",
            "+---------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CASE Statement**\n",
        "\n",
        "Case clause uses a rule to return a specific result based on the specified condition, similar to if/else statements in other programming languages"
      ],
      "metadata": {
        "id": "LbT5U36r32T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df1 = new_df.withColumn(\"Year\",expr(\"\"\"\n",
        "              CASE WHEN Year < 21 THEN Year+2000\n",
        "              WHEN Year < 100 THEN Year+1900\n",
        "              ELSE Year\n",
        "              END\"\"\"))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YzudXZFrujt",
        "outputId": "49d1e332-0873-48a0-a78b-52373a59fa8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+------+\n",
            "|     Name|Date|Month|  Year|\n",
            "+---------+----+-----+------+\n",
            "|Vaishnavi|  16|   03|1998.0|\n",
            "|  Shubham|  19|   08|  1998|\n",
            "|    Shiva|  17|   11|1997.0|\n",
            "|    Suraj|  15|   06|  1998|\n",
            "+---------+----+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The year column is string, but we are performing an arithmetic operation over it. Becuase of it spark will automatically promote it to a decimal data type and once the calucation is completed, again changes it to a string value as the column datatype is string. But the decimal points remain.\n",
        "\n",
        "To fix it there are 2 methods,\n",
        "1. **Inline casting**- cast the column to a relevant data type when you are performing an action.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BliLsyjq4_G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df1 = new_df.withColumn(\"Year\",expr(\"\"\"\n",
        "              CASE WHEN Year < 21 THEN cast(Year as int)+2000\n",
        "              WHEN Year < 100 THEN cast(Year as int)+1900\n",
        "              ELSE Year\n",
        "              END\"\"\"))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrDbi5HTryVj",
        "outputId": "3eaf9a5f-5cc6-45cd-b366-937ba43d73b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+----+\n",
            "|     Name|Date|Month|Year|\n",
            "+---------+----+-----+----+\n",
            "|Vaishnavi|  16|   03|1998|\n",
            "|  Shubham|  19|   08|1998|\n",
            "|    Shiva|  17|   11|1997|\n",
            "|    Suraj|  15|   06|1998|\n",
            "+---------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the above method or you can cast the entire expression into integertype as shown below."
      ],
      "metadata": {
        "id": "x6sxbtyC-82Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "df1  = new_df.withColumn(\"Year\",expr(\"\"\"\n",
        "              CASE WHEN Year < 21 THEN Year+2000\n",
        "              WHEN Year < 100 THEN Year+1900\n",
        "              ELSE Year\n",
        "              END\"\"\").cast(IntegerType()))\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCIkbxlm56Y1",
        "outputId": "8338ffa4-f199-4a18-8386-a6c39c6a4c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+----+\n",
            "|     Name|Date|Month|Year|\n",
            "+---------+----+-----+----+\n",
            "|Vaishnavi|  16|   03|1998|\n",
            "|  Shubham|  19|   08|1998|\n",
            "|    Shiva|  17|   11|1997|\n",
            "|    Suraj|  15|   06|1998|\n",
            "+---------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Change the schema**: We need to change the schema of the columns,or provide appropriate data types during the dataframe creation phase itself.\n",
        "Changing the schema is a choice, we have to see if it is okay to change the schema and will not affect anything.\n",
        "\n",
        "In the given scenario, changing the schema is perfectly fine."
      ],
      "metadata": {
        "id": "jf77ucou7RM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = new_df.withColumn('Date',col('Date').cast(IntegerType()))\\\n",
        "            .withColumn('Month',col('Month').cast(IntegerType()))\\\n",
        "            .withColumn('Year',col('Year').cast(IntegerType()))\n",
        "\n",
        "\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBqgDtEW7QGk",
        "outputId": "fb833c21-6c69-4edb-b6e5-2f549d272a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+----+\n",
            "|     Name|Date|Month|Year|\n",
            "+---------+----+-----+----+\n",
            "|Vaishnavi|  16|    3|  98|\n",
            "|  Shubham|  19|    8|1998|\n",
            "|    Shiva|  17|   11|  97|\n",
            "|    Suraj|  15|    6|1998|\n",
            "+---------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2.withColumn(\"Year\",expr(\"\"\"\n",
        "              CASE WHEN Year < 21 THEN Year+2000\n",
        "              WHEN Year < 100 THEN Year+1900\n",
        "              ELSE Year\n",
        "              END\"\"\"))\n",
        "df3.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfYkmb9o6KgJ",
        "outputId": "c591876b-d635-4cde-b682-e0c1c9ce808a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+----+\n",
            "|     Name|Date|Month|Year|\n",
            "+---------+----+-----+----+\n",
            "|Vaishnavi|  16|    3|1998|\n",
            "|  Shubham|  19|    8|1998|\n",
            "|    Shiva|  17|   11|1997|\n",
            "|    Suraj|  15|    6|1998|\n",
            "+---------+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: It is always recommended to cast the datatypes for columns explicitly to avoid abnormal behaviour of code.**"
      ],
      "metadata": {
        "id": "2bDstNg6Oe4X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPvDM_AV-mIF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}